<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>TaCarla: Autonomous Driving Benchmark</title>

  <!-- Theme CSS -->
  <link rel="stylesheet" href="style.css">

  <style>
    :root { --container-width: 1100px; --accent: #2563eb; }
    body { font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial; color:#111; background:#fafafa; margin:0; }
    .container { max-width: var(--container-width); margin: 0 auto; padding: 24px; }
    .hero { background: #0b1220; color: #fff; padding: 40px 0; }
    .hero-title { font-size: 2.2rem; margin: 0 0 8px; }
    .hero-subtitle { margin: 0 0 12px; opacity: 0.95; color:#dbe9ff; }
    .authors, .affiliations { display:flex; gap:12px; flex-wrap:wrap; margin-bottom:12px; font-size:0.95rem; color:#dbe9ff; }
    .btn { display:inline-block; padding:8px 14px; border-radius:8px; text-decoration:none; font-weight:600; }
    .btn-primary { background:var(--accent); color:#fff; }
    .btn-secondary { background:#e6edf8; color:#0b1220; }
    .section { padding: 28px 0; border-bottom: 1px solid #eee; background:transparent; }
    h2 { margin-top:0; }
    .figure { margin: 18px 0; text-align:center; }
    .table-responsive { overflow-x:auto; }
    .results-table { border-collapse: collapse; width: 100%; min-width: 640px; background:#fff; border-radius:6px; overflow:hidden; box-shadow:0 4px 12px rgba(0,0,0,0.04); }
    .results-table th, .results-table td { border: 1px solid #eee; padding: 8px; text-align: center; font-size:0.95rem; }
    .results-table thead th { background: #fbfdff; font-weight:700; }
    .citation-box { background:#fff; border:1px solid #eef5ff; padding:12px; border-radius:8px; }
    .image-modal { display:none; position:fixed; inset:0; background: rgba(0,0,0,0.75); align-items:center; justify-content:center; z-index:9999; }
    .image-modal.active { display:flex; }
    .image-modal img { max-width:96%; max-height:92%; border-radius:6px; box-shadow: 0 8px 30px rgba(0,0,0,0.6); }
    .close-modal { position:absolute; top:20px; right:26px; font-size:34px; color:#fff; cursor:pointer; }
    .small-muted { color:#666; font-size:0.9rem; }
    .content-card { background:#fff; border:1px solid #eee; padding:14px; border-radius:8px; box-shadow:0 3px 10px rgba(0,0,0,0.03); }
    .card-title { font-weight:600; margin-bottom:8px; }

    /* Study Highlights grid */
    .highlights-grid {
      display:grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap:18px;
      margin-top:12px;
      align-items:start;
    }
    .highlight-card {
      background:#fff;
      border:1px solid #eef3ff;
      padding:14px;
      border-radius:10px;
      box-shadow:0 6px 18px rgba(0,0,0,0.04);
      display:flex;
      gap:12px;
      align-items:flex-start;
    }
    .highlight-icon {
      width:44px; height:44px; flex:0 0 44px; border-radius:8px; display:flex; align-items:center; justify-content:center; font-size:20px;
      background:linear-gradient(180deg, rgba(37,99,235,0.12), rgba(37,99,235,0.06)); color:var(--accent);
    }
    .highlight-body h3 { margin:0 0 6px; font-size:1rem; }
    .highlight-body p { margin:0; font-size:0.95rem; color:#444; line-height:1.4; }

    /* Data visualization gallery */
    .viz-gallery { display:flex; gap:16px; flex-wrap:wrap; justify-content:center; align-items:flex-start; margin-top:12px; }
    .viz-gallery .viz-item { flex: 1 1 300px; max-width: 360px; text-align:center; background:#fff; padding:8px; border-radius:8px; box-shadow:0 6px 18px rgba(0,0,0,0.04); }
    .viz-gallery img { width:100%; height:auto; border-radius:6px; box-shadow:0 4px 12px rgba(0,0,0,0.04); cursor:zoom-in; display:block; }
    .viz-caption { margin-top:8px; font-size:0.95rem; color:#444; }

    /* Video gallery (new) */
    .video-gallery { display:flex; gap:16px; flex-wrap:wrap; justify-content:center; align-items:flex-start; margin-top:16px; }
    .video-gallery .video-item { flex: 1 1 300px; max-width: 480px; text-align:center; background:#fff; padding:8px; border-radius:8px; box-shadow:0 6px 18px rgba(0,0,0,0.04); }
    .video-gallery video { width:100%; height:auto; border-radius:6px; box-shadow:0 4px 12px rgba(0,0,0,0.04); display:block; }
    .video-caption { margin-top:8px; font-size:0.95rem; color:#444; }

    /* Traffic-light samples */
    .tl-samples { display:flex; gap:16px; flex-wrap:wrap; justify-content:center; align-items:center; margin-top:12px; }
    .tl-samples img { width:48%; max-width:520px; border-radius:6px; box-shadow:0 6px 18px rgba(0,0,0,0.06); cursor:zoom-in; display:block; }

    /* Responsive tweaks */
    @media (max-width:920px){ .tl-samples img { width:60%; max-width:420px; } }
    @media (max-width:720px){ .hero-title { font-size:1.6rem; } .container { padding:16px; } .viz-gallery .viz-item { max-width:100%; flex: 1 1 100%; } .results-table { font-size:0.88rem; } .video-gallery .video-item { max-width:100%; flex:1 1 100%; } }
  </style>
</head>
<body>
  <!-- Header -->
  <header class="hero" role="banner">
    <div class="container">
      <div class="hero-content">
        <h1 class="hero-title">TaCarla</h1>
        <p class="hero-subtitle">A comprehensive benchmarking dataset for end-to-end autonomous driving</p>

        <div class="authors" aria-hidden="false">
          <span class="author">TaCarla Research Team</span>
          <span class="author">CARLA Leaderboard 2.0</span>
        </div>

        <div class="affiliations">
          <span class="affiliation">CARLA Simulation ‚Äî Leaderboard 2.0 Scenarios</span>
        </div>

        <div class="action-buttons" style="margin-top:12px;">
          <a class="btn btn-primary" href="#download">üì• Download</a>
          <a class="btn btn-secondary" href="#citation">üìã Cite</a>
          <a class="btn btn-secondary" href="https://github.com/atg93/TaCarla-Visualization" target="_blank" rel="noopener">üìä Data Visualization</a>
        </div>
      </div>
    </div>
  </header>

  <!-- ABSTRACT -->
  <section class="section" id="abstract">
    <div class="container">
      <h2>Abstract</h2>
      <div class="content-card" aria-live="polite">
        <div class="card-title"><span>Abstract</span></div>
        <div id="abstract-content" class="content">
Collecting a high-quality dataset is a critical task that demands meticulous attention to detail, as overlooking certain aspects can render the entire dataset unusable. Autonomous driving challenges remain a prominent area of research, requiring further exploration to enhance the perception and planning performance of vehicles. However, existing datasets are often incomplete. For instance, datasets that include perception information generally lack planning data, while planning datasets typically consist of extensive driving sequences where the ego vehicle predominantly drives forward, offering limited behavioral diversity.

The CARLA Leaderboard 2.0 challenge, providing a diverse set of scenarios to address the long-tail problem in autonomous driving, has emerged as a valuable alternative platform for developing perception and planning models. Nevertheless, existing datasets collected on this platform present certain limitations. Some datasets appear to be tailored primarily for limited sensor configuration, with particular sensor configurations. Additionally, in some datasets, the expert policies used for data collection exhibit suboptimal driving behaviors, such as oscillations.

To support end-to-end autonomous driving research, we have collected a new dataset comprising over 2.85 million frames using the CARLA simulation environment for the diverse Leaderboard 2.0 challenge scenarios, making it the largest dataset in the literature to the best of our knowledge. Our dataset is designed not only for planning tasks but also supports dynamic object detection, lane divider detection, centerline detection, traffic light recognition, and prediction tasks. Furthermore, we demonstrate its versatility by training various models using our dataset.
        </div>
      </div>
      <p class="small-muted" style="margin-top:10px">Note: Any math expressions will appear as plain text unless you add a math renderer.</p>
    </div>
  </section>

  <!-- Study Highlights -->
  <section class="section" id="study-highlights">
    <div class="container">
      <h2>Study Highlights</h2>
      <div class="highlights-grid" role="list">
        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üì¶</div>
          <div class="highlight-body">
            <h3>Largest Dataset</h3>
            <p>The largest dataset collected for the CARLA Leaderboard 2.0 challenge ‚Äî designed to cover diverse scenarios and long-tail events.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üèÅ</div>
          <div class="highlight-body">
            <h3>Benchmark</h3>
            <p>Provides benchmarks for multiple autonomous-driving tasks: <strong>object detection</strong>, <strong>lane detection</strong>, <strong>traffic light detection</strong>, and <strong>planning</strong>.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üìù</div>
          <div class="highlight-body">
            <h3>Text</h3>
            <p>Per-frame textual descriptions explaining the current situation of the ego vehicle (lane, speed, nearby hazards, etc.) to support behavior understanding and language-conditioned tasks.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üìä</div>
          <div class="highlight-body">
            <h3>Score</h3>
            <p>A scene-level rarity score that measures how rare or unique a scene is in the dataset ‚Äî useful for curriculum learning and focused evaluation on rare events.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üíæ</div>
          <div class="highlight-body">
            <h3>Parquet data format</h3>
            <p>Annotations and extracted features are provided in Parquet format for faster I/O and training throughput; integrates easily with modern data pipelines.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- EXPERIMENTS -->
  <section class="section" id="experiments">
    <div class="container">
      <h2 id="sec:experiments">Experiments</h2>

      <!-- 3D Object Detection -->
      <article class="experiment-block" id="3d-object-detection">
        <h3>3D Object Detection</h3>

        <p>
          We employed a non-transformer-based architecture for multi-view bird‚Äôs eye view (BEV)-based 3D object detection. In this architecture, the multi-view camera images are initially processed by a convolutional image encoder, specifically <strong>RegNetY-800MF</strong>, with a feature pyramid network based on <strong>BiFPN</strong>. We project the feature levels at /8, /16, and /32 resolutions into the BEV representation using Lift-Splat projection. Features from the previous two frames are warped to the current frame using egomotion and then concatenated along the channel dimension, similar to BevDet4D. Gradients produced by the previous frames are not used to update the image encoder. The resulting spatio-temporal BEV features are processed by a ResNet-based BEV backbone. These features are then shared among task-specific heads.
        </p>

        <p>
          We utilize <strong>RQR3D</strong> for BEV-based 3D object detection. RQR3D reparametrizes the regression targets for the 3D bounding boxes and implements this reparameterized regression task on an anchor-free single-stage object detector, introducing an objectness head to address class imbalance problems of single-stage object detectors. RQR3D outperforms widely-adopted CenterPoint-based approaches, yielding lower translation and orientation errors, which are crucial for safe autonomous driving. When using LiDAR, we simply map the point cloud onto the BEV grid and concatenate it with the projected image feature before temporal processing.
        </p>

        <!-- Table: class counts -->
        <figure class="table-figure" aria-labelledby="tab-3d-class-counts-caption">
          <figcaption id="tab-3d-class-counts-caption"><strong>Table:</strong> The number of objects for 3D object detection</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:3d_class_counts">
              <thead>
                <tr>
                  <th></th>
                  <th>Ambulance</th>
                  <th>Construction</th>
                  <th>Crossbike</th>
                  <th>Walker</th>
                  <th>Car</th>
                  <th>Firetruck</th>
                  <th>Police</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th>train</th>
                  <td>2,306</td>
                  <td>50,486</td>
                  <td>28,980</td>
                  <td>17,247</td>
                  <td>683,332</td>
                  <td>1,070</td>
                  <td>21,181</td>
                </tr>
                <tr>
                  <th>val</th>
                  <td>2,060</td>
                  <td>2,248</td>
                  <td>32,177</td>
                  <td>9,703</td>
                  <td>419,697</td>
                  <td>1,495</td>
                  <td>9,803</td>
                </tr>
              </tbody>
            </table>
          </div>
        </figure>

        <p>
          To align our dataset with the <em>nuScenes</em> benchmark, which provides annotated keyframes at 2 Hz, we downsampled our original 10 Hz data to 2 Hz. This conversion ensures consistency in temporal resolution, facilitating fair comparisons and compatibility with existing evaluation protocols.
        </p>

        <p>
          Additionally, we selected scenarios whose name contains keywords such as <em>accident, construction, dynamic, pedestrian, hazard, emergency,</em> and <em>opposite</em> in order to achieve a more balanced class distribution. The resulting number of objects per category is summarized above.
        </p>

        <p>
          We utilize two different sensor configurations: camera-only and camera‚ÄìLiDAR. Evaluation metrics are adopted from nuScenes, including mean Average Precision (mAP), Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE) and Average Velocity Error (AVE), excluding Average Attribute Error (AAE) as it is not applicable for TaCarla. These metrics provide a comprehensive assessment of detection performance across various object classes. The inclusion of LiDAR modality enhances depth estimation accuracy, leading to improved localization and orientation predictions, as reflected in lower ATE and AOE values. Conversely, the camera-only configuration exhibits higher errors due to the inherent challenges in depth estimation. The detailed class-wise performance metrics are presented below, illustrating the comparative effectiveness of both training approaches.
        </p>

        <!-- Camera-only results table -->
        <figure class="table-figure" aria-labelledby="tab-3d-results-lss-caption">
          <figcaption id="tab-3d-results-lss-caption"><strong>Table:</strong> Camera-only 3D object detection performance</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:3d_results_lss">
              <thead>
                <tr><th></th><th>AP</th><th>ATE</th><th>ASE</th><th>AOE</th><th>AVE</th></tr>
              </thead>
              <tbody>
                <tr><th>Car</th><td>0.459</td><td>0.444</td><td>0.147</td><td>0.012</td><td>0.559</td></tr>
                <tr><th>Crossbike</th><td>0.324</td><td>0.242</td><td>0.094</td><td>0.057</td><td>0.165</td></tr>
                <tr><th>Walker</th><td>0.426</td><td>0.456</td><td>0.885</td><td>1.333</td><td>0.292</td></tr>
                <tr><th>Police</th><td>0.381</td><td>0.249</td><td>0.056</td><td>0.011</td><td>0.048</td></tr>
                <tr><th>Construction</th><td>0.419</td><td>0.665</td><td>0.812</td><td>1.125</td><td>0.065</td></tr>
                <tr><th>Ambulance</th><td>0.098</td><td>0.440</td><td>0.132</td><td>0.065</td><td>0.525</td></tr>
                <tr><th>Firetruck</th><td>0.140</td><td>0.487</td><td>0.155</td><td>0.004</td><td>0.618</td></tr>
                <tr><th>Average</th><td>mAP: 0.32</td><td>mATE: 0.43</td><td>mASE: 0.33</td><td>mAOE: 0.37</td><td>mAVE: 0.32</td></tr>
              </tbody>
            </table>
          </div>
        </figure>

        <!-- Camera+LiDAR results table -->
        <figure class="table-figure" aria-labelledby="tab-3d-results-lss-lidar-caption">
          <figcaption id="tab-3d-results-lss-lidar-caption"><strong>Table:</strong> Camera‚ÄìLiDAR 3D object detection performance</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:3d_results_lss_lidar">
              <thead>
                <tr><th></th><th>AP</th><th>ATE</th><th>ASE</th><th>AOE</th><th>AVE</th></tr>
              </thead>
              <tbody>
                <tr><th>Car</th><td>0.716</td><td>0.173</td><td>0.125</td><td>0.022</td><td>0.399</td></tr>
                <tr><th>Crossbike</th><td>0.556</td><td>0.113</td><td>0.086</td><td>0.076</td><td>0.119</td></tr>
                <tr><th>Walker</th><td>0.527</td><td>0.152</td><td>0.885</td><td>1.304</td><td>0.277</td></tr>
                <tr><th>Police</th><td>0.486</td><td>0.082</td><td>0.052</td><td>0.018</td><td>0.038</td></tr>
                <tr><th>Construction</th><td>0.657</td><td>0.253</td><td>0.821</td><td>1.125</td><td>0.098</td></tr>
                <tr><th>Ambulance</th><td>0.428</td><td>0.254</td><td>0.100</td><td>0.074</td><td>0.283</td></tr>
                <tr><th>Firetruck</th><td>0.452</td><td>0.270</td><td>0.108</td><td>0.002</td><td>0.344</td></tr>
                <tr><th>Average</th><td>mAP: 0.55</td><td>mATE: 0.19</td><td>mASE: 0.31</td><td>mAOE: 0.37</td><td>mAVE: 0.22</td></tr>
              </tbody>
            </table>
          </div>
        </figure>

      </article>

      <!-- Lane Detection -->
      <article class="experiment-block" id="lane-detection">
        <h3>Lane Detection</h3>

        <p>
          Lane detection consists of two sub-tasks which are lane divider detection and centerline detection. For the lane divider and centerline detection tasks, Chamfer Distance-based Average Precision (<code>AP<sub>c</sub></code>) and Fr√©chet Distance-based Average Precision (<code>AP<sub>f</sub></code>) metrics were utilized. These metrics are commonly used in the literature to evaluate the geometric similarity between predicted and ground-truth polylines. For <code>AP<sub>f</sub></code>, the thresholds are 1, 2, and 3 meters, and for <code>AP<sub>c</sub></code>, the thresholds are 0.5, 1, and 1.5 meters. Additionally, the F1 metric is included, which is another widely used metric in the literature for assessing lane detection performance. For F1 metric calculation, if 75% of the points are within the predetermined threshold, the instances are assumed to be true positives. In this study, this threshold is set to 1.5 meters. For all evaluation metrics, 11 ground truth points are utilized.
        </p>

        <p>
          For the training of both centerlines and lane dividers, the TopoBDA study was utilized, which incorporates specialized attention structures and advanced polyline training practices derived from TopoMaskV2. The Bezier Deformable Attention mechanism, a key innovation of TopoBDA, focuses attention around Bezier keypoints rather than a single central point. This approach enhances the efficiency of polyline learning by improving the detection and representation of elongated and thin polyline structures.
        </p>

        <figure class="table-figure" aria-labelledby="tab-topobda-caption">
          <figcaption id="tab-topobda-caption"><strong>Table:</strong> Centerline and Lane Divider Detection Results of TopoBDA architecture for TaCarla Dataset.</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab_topobda_tacarla">
              <thead><tr><th>Detection Task</th><th>AP<sub>f</sub></th><th>AP<sub>c</sub></th><th>F1<sub>1.5</sub></th></tr></thead>
              <tbody>
                <tr><td>Centerline Detection</td><td>58.2</td><td>60.9</td><td>73.8</td></tr>
                <tr><td>Lane Divider Detection</td><td>N/A</td><td>60.2</td><td>75.6</td></tr>
              </tbody>
            </table>
          </div>
        </figure>

        <!-- BEV image (figures) -->
        <figure class="figure" id="fig:bev_samples">
          <img src="images/bev_tacarla.png" alt="BEV results (images/bev_tacarla.png)" data-full="images/bev_tacarla.png" style="width:100%;max-width:900px;cursor:zoom-in;border-radius:6px;">
          <figcaption>Bird's Eye View (BEV) results demonstrating TopoBDA performance. GT = ground truth, Pred = prediction, GT+Pred = overlay.</figcaption>
        </figure>

      </article>

      <!-- Traffic Light Detection -->
      <article class="experiment-block" id="traffic-light-detection">
        <h3>Traffic Light Detection</h3>

        <p>
          The dataset we propose consists of 238,780 and 187,987 images containing traffic light instances in the training and validation sets, respectively. Every individual image in the dataset consists of a single traffic light instance with three distinct classes; <em>red</em>, <em>yellow</em>, <em>green</em>. Every instance is labeled with their 2D bounding box and corresponding class. For the traffic light detection task, we employed off-the-shelf single-stage object detector <strong>FCOS</strong> with ResNet-50 backbone as a baseline. We use 1√ó training schedule; we train for 12 epochs with a learning rate of <code>1e-3</code>. We schedule the learning rate at 8th and 11th epochs with a factor of 0.1. We report COCO-style <code>AP</code> and <code>AP<sub>50</sub></code> below.
        </p>

        <figure class="table-figure" aria-labelledby="tab-tlr-caption">
          <figcaption id="tab-tlr-caption"><strong>Table:</strong> Traffic Light Detection task results in TaCarla.</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab_TLR">
              <thead><tr><th>Model</th><th>AP</th><th>AP<sub>50</sub></th></tr></thead>
              <tbody><tr><td>FCOS</td><td>59.5</td><td>88.2</td></tr></tbody>
            </table>
          </div>
        </figure>

        <!-- traffic light images (images/) -->
        <figure class="figure tl-samples" role="group" aria-label="Traffic light model outputs">
          <img src="images/tl_red.jpg" alt="Traffic light red example (images/tl_red.jpg)" data-full="images/tl_red.jpg">
          <img src="images/tl_green.jpg" alt="Traffic light green example (images/tl_green.jpg)" data-full="images/tl_green.jpg">
          <figcaption style="width:100%;margin-top:10px;text-align:center;">Outputs of the FCOS traffic light model (red and green examples).</figcaption>
        </figure>

      </article>

      <!-- Planning -->
      <article class="experiment-block" id="planning">
        <h3>Planning</h3>

        <figure class="figure">
          <img src="images/planning.png" alt="Waypoints: GT, PlanT, Transfuser, DiffusionDrive (images/planning.png)" data-full="images/planning.png" style="width:100%;max-width:900px;border-radius:6px;">
          <figcaption>Waypoints from ground truth (Top Left), PlanT (Top Right), Transfuser (Bottom Left), and DiffusionDrive (Bottom Right).</figcaption>
        </figure>

        <p>
          For the planning task, we trained three baseline agents: <strong>Transfuser</strong>, <strong>DiffusionDrive</strong>, and <strong>PlanT</strong>.
          We chose Transfuser and DiffusionDrive owing to their success on Navsim. Both agents use the same ResNet-34 backbone and take 3 forward-facing cameras plus LiDAR. Cameras were cropped and concatenated into an image of size <code>256√ó1024</code> and LiDAR rasterized into BEV <code>256√ó256</code>. Ego status input includes velocity, acceleration and driving command (as in Navsim). Lane-guidance waypoints were annotated and the point 15 m ahead is classified as left/right/straight using a 2 m lateral threshold.
        </p>

        <p>
          DiffusionDrive was trained for 6 epochs and Transfuser for 3 epochs with learning rate <code>7.5√ó10<sup>-5</sup></code> on 8 NVIDIA A100 GPUs with total batch size 64. Training scenarios were filtered with driving score &gt; 70. We sampled ground-truth trajectories at 2 Hz (8 waypoints for 4 s horizon). Implementation follows DiffusionDrive (20 anchors clustered from dataset); evaluation uses 2 denoising steps as in Navsim.
        </p>

        <p>
          PlanT was trained for 50 epochs, batch size 16, learning rate <code>1e-4</code>. Because PlanT uses ground-truth information, training it helps verify dataset labels.
        </p>

        <p>
          For evaluation we report open-loop and closed-loop metrics. Open-loop metrics are Average Displacement Error (ADE), Final Displacement Error (FDE), Average Heading Error (AHE) and Final Heading Error (FHE) following NuPlan conventions. Metrics are provided for 1 s, 2 s and 4 s horizons sampled at 2 Hz. Closed-loop evaluation was run on simplified validation routes using CARLA Leaderboard V2; scenario-specific results are in the appendix.
        </p>

        <!-- Open-loop table -->
        <figure class="table-figure" aria-labelledby="tab-openloop-caption">
          <figcaption id="tab-openloop-caption"><strong>Table:</strong> Open-loop results on validation set with 1s, 2s and 4s planning horizons (H).</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab_openloop_results">
              <thead>
                <tr>
                  <th rowspan="2">Model</th>
                  <th colspan="5">Input</th>
                  <th rowspan="2">H (s)</th>
                  <th colspan="4">Results</th>
                </tr>
                <tr>
                  <th>Camera</th><th>LiDAR</th><th>GT Box</th><th>Ego Status</th><th></th>
                  <th>ADE</th><th>FDE</th><th>AHE</th><th>FHE</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="3">DiffusionDrive</td>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>4</td><td>2.69</td><td>5.58</td><td>0.27</td><td>0.21</td>
                </tr>
                <tr>
                  <td>‚úì</td><td>‚úì</td><td></td><td>‚úì</td><td></td>
                  <td>2</td><td>1.14</td><td>2.14</td><td>0.21</td><td>0.21</td>
                </tr>
                <tr>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>1</td><td>0.51</td><td>0.78</td><td>0.12</td><td>0.12</td>
                </tr>

                <tr>
                  <td rowspan="3">PlanT</td>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>4</td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td>
                </tr>
                <tr>
                  <td></td><td></td><td>‚úì</td><td></td><td></td>
                  <td>2</td><td>1.03</td><td>1.71</td><td>0.36</td><td>0.34</td>
                </tr>
                <tr>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>1</td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td>
                </tr>

                <tr>
                  <td rowspan="3">Transfuser</td>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>4</td><td>2.29</td><td>4.97</td><td>0.23</td><td>0.27</td>
                </tr>
                <tr>
                  <td>‚úì</td><td>‚úì</td><td></td><td>‚úì</td><td></td>
                  <td>2</td><td>0.91</td><td>1.74</td><td>0.23</td><td>0.27</td>
                </tr>
                <tr>
                  <td></td><td></td><td></td><td></td><td></td>
                  <td>1</td><td>0.40</td><td>0.59</td><td>0.22</td><td>0.22</td>
                </tr>
              </tbody>
            </table>
          </div>
        </figure>

      </article>

    </div>
  </section>

  <!-- Visualization: show three images in specified order from images/ folder -->
  <section class="section" id="visualization">
    <div class="container">
      <h2>Visualization</h2>
      <p class="small-muted">Three data visualization images (in this order) from the <code>images/</code> folder.</p>

      <div class="viz-gallery" role="list">
        <div class="viz-item" role="listitem">
          <img src="images/select_episode.png" alt="Select Episode (images/select_episode.png)" data-full="images/select_episode.png" loading="lazy">
          <div class="viz-caption">Select Episode</div>
        </div>

        <div class="viz-item" role="listitem">
          <img src="images/bev.png" alt="BEV Visualization (images/bev.png)" data-full="images/bev.png" loading="lazy">
          <div class="viz-caption">BEV Visualization</div>
        </div>

        <div class="viz-item" role="listitem">
          <img src="images/front.png" alt="Front Camera View (images/front.png)" data-full="images/front.png" loading="lazy">
          <div class="viz-caption">Front Camera View</div>
        </div>
      </div>

      <!-- NEW: Video gallery -->
      <p class="small-muted" style="margin-top:18px">Two demonstration videos from the <code>videos/</code> folder.</p>
      <div class="video-gallery" role="list">
        <div class="video-item" role="listitem">
          <video controls preload="metadata" poster="images/video1_poster.png">
            <source src="videos/video1.mp4" type="video/mp4">
            Your browser does not support the video tag. You can <a href="videos/Town12_radar_HazardAtSideLaneTwoWays.mp4">download video1.mp4</a> instead.
          </video>
          <div class="video-caption">HazardAtSideLaneTwoWays</div>
        </div>

        <div class="video-item" role="listitem">
          <video controls preload="metadata" poster="images/video2_poster.png">
            <source src="videos/video2.mp4" type="video/mp4">
            Your browser does not support the video tag. You can <a href="videos/Town12_radar_StaticCutIn.mp4">download video2.mp4</a> instead.
          </video>
          <div class="video-caption">StaticCutIn</div>
        </div>
      </div>

    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="citation">
    <div class="container">
      <h2>Citation</h2>
      <div class="citation-box">
        <pre id="citation-text">@inproceedings{2025tacarla,
  title   = {TaCarla: A comprehensive benchmarking dataset for end-to-end autonomous driving},
  author  = {TaCarla Research Team},
  year    = {2025}
}</pre>
        <div style="display:flex;flex-direction:column;gap:8px; margin-top:8px;">
          <button class="btn btn-primary" id="copyCitation">üìã Copy Citation</button>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer" style="padding:18px 0;background:#fff;border-top:1px solid #eee;">
    <div class="container" style="display:flex;gap:12px;justify-content:space-between;align-items:center;">
      <div>¬© 2025 TaCarla Team</div>
      <div>Contact: <a href="mailto:team@tacarla.example">team@tacarla.example</a></div>
    </div>
  </footer>

  <!-- Image modal -->
  <div id="imageModal" class="image-modal" role="dialog" aria-hidden="true">
    <div class="close-modal" id="closeModal" title="Close">&times;</div>
    <img id="modalImage" src="" alt="Full size" />
  </div>

  <!-- Inline script: image modal + copy citation -->
  <script>
    // image modal
    (function(){
      document.querySelectorAll('img[data-full]').forEach(img=>{
        img.style.cursor = 'zoom-in';
        img.addEventListener('click', ()=>{
          const modal = document.getElementById('imageModal');
          const modalImg = document.getElementById('modalImage');
          modalImg.src = img.dataset.full || img.src;
          modal.classList.add('active');
          modal.setAttribute('aria-hidden','false');
          document.body.style.overflow = 'hidden';
        });
      });
      const closeBtn = document.getElementById('closeModal');
      if (closeBtn) closeBtn.addEventListener('click', ()=>{
        const modal = document.getElementById('imageModal');
        modal.classList.remove('active');
        modal.setAttribute('aria-hidden','true');
        document.getElementById('modalImage').src = '';
        document.body.style.overflow = '';
      });
      document.addEventListener('keydown', (e)=>{ if (e.key==='Escape'){
        const modal=document.getElementById('imageModal'); if(modal && modal.classList.contains('active')){
          modal.classList.remove('active'); modal.setAttribute('aria-hidden','true'); document.getElementById('modalImage').src=''; document.body.style.overflow='';
        }
      }});
      // clicking outside image closes modal
      const imgModal = document.getElementById('imageModal');
      imgModal && imgModal.addEventListener('click', (e)=>{ if (e.target === imgModal){ imgModal.classList.remove('active'); imgModal.setAttribute('aria-hidden','true'); document.getElementById('modalImage').src=''; document.body.style.overflow=''; } });
    })();

    // copy citation
    (function(){
      const btn = document.getElementById('copyCitation');
      if (!btn) return;
      btn.addEventListener('click', async ()=>{
        const text = document.getElementById('citation-text').innerText;
        try {
          await navigator.clipboard.writeText(text);
          btn.textContent = '‚úÖ Copied!';
          setTimeout(()=> btn.textContent = 'üìã Copy Citation', 1500);
        } catch (err){
          alert('Could not copy automatically. Here is the citation:\n\n' + text);
        }
      });
    })();
  </script>
</body>
</html>
