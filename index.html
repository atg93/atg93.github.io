<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>TaCarla: Autonomous Driving Benchmark</title>

  <!-- Theme CSS -->
  <link rel="stylesheet" href="style.css">

  <style>
    :root { --container-width: 1100px; --accent: #2563eb; }
    body { font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial; color:#111; background:#fafafa; margin:0; }
    .container { max-width: var(--container-width); margin: 0 auto; padding: 24px; }
    .hero { background: #0b1220; color: #fff; padding: 40px 0; }
    .hero-title { font-size: 2.2rem; margin: 0 0 8px; }
    .hero-subtitle { margin: 0 0 12px; opacity: 0.95; color:#dbe9ff; }
    .authors, .affiliations { display:flex; gap:12px; flex-wrap:wrap; margin-bottom:12px; font-size:0.95rem; color:#dbe9ff; }
    .btn { display:inline-block; padding:8px 14px; border-radius:8px; text-decoration:none; font-weight:600; }
    .btn-primary { background:var(--accent); color:#fff; }
    .btn-secondary { background:#e6edf8; color:#0b1220; }
    .section { padding: 28px 0; border-bottom: 1px solid #eee; background:transparent; }
    h2 { margin-top:0; }
    .figure { margin: 18px 0; text-align:center; }
    .table-responsive { overflow-x:auto; }
    .results-table { border-collapse: collapse; width: 100%; min-width: 640px; background:#fff; border-radius:6px; overflow:hidden; box-shadow:0 4px 12px rgba(0,0,0,0.04); }
    .results-table th, .results-table td { border: 1px solid #eee; padding: 8px; text-align: center; font-size:0.95rem; }
    .results-table thead th { background: #fbfdff; font-weight:700; }
    .citation-box { background:#fff; border:1px solid #eef5ff; padding:12px; border-radius:8px; }
    .image-modal { display:none; position:fixed; inset:0; background: rgba(0,0,0,0.75); align-items:center; justify-content:center; z-index:9999; }
    .image-modal.active { display:flex; }
    .image-modal img { max-width:96%; max-height:92%; border-radius:6px; box-shadow: 0 8px 30px rgba(0,0,0,0.6); }
    .close-modal { position:absolute; top:20px; right:26px; font-size:34px; color:#fff; cursor:pointer; }
    .small-muted { color:#666; font-size:0.9rem; }
    .content-card { background:#fff; border:1px solid #eee; padding:14px; border-radius:8px; box-shadow:0 3px 10px rgba(0,0,0,0.03); }
    .card-title { font-weight:600; margin-bottom:8px; }

    /* Study Highlights grid */
    .highlights-grid {
      display:grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap:18px;
      margin-top:12px;
      align-items:start;
    }
    .highlight-card {
      background:#fff;
      border:1px solid #eef3ff;
      padding:14px;
      border-radius:10px;
      box-shadow:0 6px 18px rgba(0,0,0,0.04);
      display:flex;
      gap:12px;
      align-items:flex-start;
    }
    .highlight-icon {
      width:44px; height:44px; flex:0 0 44px; border-radius:8px; display:flex; align-items:center; justify-content:center; font-size:20px;
      background:linear-gradient(180deg, rgba(37,99,235,0.12), rgba(37,99,235,0.06)); color:var(--accent);
    }
    .highlight-body h3 { margin:0 0 6px; font-size:1rem; }
    .highlight-body p { margin:0; font-size:0.95rem; color:#444; line-height:1.4; }

    /* Data visualization gallery */
    .viz-gallery { display:flex; gap:16px; flex-wrap:wrap; justify-content:center; align-items:flex-start; margin-top:12px; }
    .viz-gallery .viz-item { flex: 1 1 300px; max-width: 360px; text-align:center; background:#fff; padding:8px; border-radius:8px; box-shadow:0 6px 18px rgba(0,0,0,0.04); }
    .viz-gallery img { width:100%; height:auto; border-radius:6px; box-shadow:0 4px 12px rgba(0,0,0,0.04); cursor:zoom-in; display:block; }
    .viz-caption { margin-top:8px; font-size:0.95rem; color:#444; }

    /* Traffic-light samples */
    .tl-samples { display:flex; gap:16px; flex-wrap:wrap; justify-content:center; align-items:center; margin-top:12px; }
    .tl-samples img { width:48%; max-width:520px; border-radius:6px; box-shadow:0 6px 18px rgba(0,0,0,0.06); cursor:zoom-in; display:block; }

    /* Responsive tweaks */
    @media (max-width:920px){ .tl-samples img { width:60%; max-width:420px; } }
    @media (max-width:720px){ .hero-title { font-size:1.6rem; } .container { padding:16px; } .viz-gallery .viz-item { max-width:100%; flex: 1 1 100%; } .results-table { font-size:0.88rem; } }
  </style>
</head>
<body>
  <!-- Header -->
  <header class="hero" role="banner">
    <div class="container">
      <div class="hero-content">
        <h1 class="hero-title">TaCarla</h1>
        <p class="hero-subtitle">A comprehensive benchmarking dataset for end-to-end autonomous driving</p>

        <div class="authors" aria-hidden="false">
          <span class="author">TaCarla Research Team</span>
          <span class="author">CARLA Leaderboard 2.0</span>
        </div>

        <div class="affiliations">
          <span class="affiliation">CARLA Simulation ‚Äî Leaderboard 2.0 Scenarios</span>
        </div>

        <div class="action-buttons" style="margin-top:12px;">
          <a class="btn btn-primary" href="#download">üì• Download</a>
          <a class="btn btn-secondary" href="#citation">üìã Cite</a>
          <a class="btn btn-secondary" href="https://github.com/atg93/TaCarla-Visualization" target="_blank" rel="noopener">üìä Data Visualization</a>
        </div>
      </div>
    </div>
  </header>

  <!-- ABSTRACT -->
  <section class="section" id="abstract">
    <div class="container">
      <h2>Abstract</h2>
      <div class="content-card" aria-live="polite">
        <div class="card-title"><span>Abstract</span></div>
        <div id="abstract-content" class="content">
Collecting a high-quality dataset is a critical task that demands meticulous attention to detail, as overlooking certain aspects can render the entire dataset unusable. Autonomous driving challenges remain a prominent area of research, requiring further exploration to enhance the perception and planning performance of vehicles. However, existing datasets are often incomplete. For instance, datasets that include perception information generally lack planning data, while planning datasets typically consist of extensive driving sequences where the ego vehicle predominantly drives forward, offering limited behavioral diversity.

The CARLA Leaderboard 2.0 challenge, providing a diverse set of scenarios to address the long-tail problem in autonomous driving, has emerged as a valuable alternative platform for developing perception and planning models. Nevertheless, existing datasets collected on this platform present certain limitations. Some datasets appear to be tailored primarily for limited sensor configuration, with particular sensor configurations. Additionally, in some datasets, the expert policies used for data collection exhibit suboptimal driving behaviors, such as oscillations.

To support end-to-end autonomous driving research, we have collected a new dataset comprising over 2.85 million frames using the CARLA simulation environment for the diverse Leaderboard 2.0 challenge scenarios, making it the largest dataset in the literature to the best of our knowledge. Our dataset is designed not only for planning tasks but also supports dynamic object detection, lane divider detection, centerline detection, traffic light recognition, and prediction tasks. Furthermore, we demonstrate its versatility by training various models using our dataset.
        </div>
      </div>
      <p class="small-muted" style="margin-top:10px">Note: Any math expressions will appear as plain text unless you add a math renderer.</p>
    </div>
  </section>

  <!-- Study Highlights -->
  <section class="section" id="study-highlights">
    <div class="container">
      <h2>Study Highlights</h2>
      <div class="highlights-grid" role="list">
        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üì¶</div>
          <div class="highlight-body">
            <h3>Largest CARLA Leaderboard Dataset</h3>
            <p>Over <strong>2.85 million frames</strong> from CARLA Leaderboard 2.0 scenarios ‚Äî to our knowledge the largest dataset covering perception and planning together.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üß©</div>
          <div class="highlight-body">
            <h3>Multi-task & Multi-modal</h3>
            <p>Supports dynamic object detection, lane divider & centerline detection, traffic light recognition and trajectory prediction with camera-only and camera+LiDAR setups.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üéØ</div>
          <div class="highlight-body">
            <h3>Long-tail Scenario Coverage</h3>
            <p>Scenarios selected for long-tail events (accident, construction, emergency, hazards, opposite direction, etc.) to improve robustness on rare cases.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üî¨</div>
          <div class="highlight-body">
            <h3>Benchmarked Baselines</h3>
            <p>Benchmarked strong baselines (BEV detectors, Transfuser, DiffusionDrive, PlanT) and provided open-loop & closed-loop evaluations.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üîÅ</div>
          <div class="highlight-body">
            <h3>Standardized Temporal Resolution</h3>
            <p>Downsampled original 10Hz data to 2Hz to align with nuScenes-style keyframes and common evaluation pipelines.</p>
          </div>
        </div>

        <div class="highlight-card" role="listitem">
          <div class="highlight-icon">üìà</div>
          <div class="highlight-body">
            <h3>Research & Validation Ready</h3>
            <p>Designed for both offline training/analysis and closed-loop validation on CARLA Leaderboard V2 ‚Äî useful for training, testing and behavior analysis.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- EXPERIMENTS (3D detection, lane, traffic light, planning) -->
  <section class="section" id="experiments">
    <div class="container">
      <h2 id="sec:experiments">Experiments</h2>

      <!-- 3D Object Detection -->
      <article class="experiment-block" id="3d-object-detection">
        <h3>3D Object Detection</h3>
        <p>
          We employed a non-transformer-based architecture for multi-view BEV 3D object detection. The multi-view camera images
          are processed by a RegNetY-800MF encoder with BiFPN; features ( /8, /16, /32 ) are projected to BEV using Lift-Splat.
          Temporal frames are warped with egomotion and concatenated (like BevDet4D). BEV backbone is ResNet-based and task heads include RQR3D.
        </p>

        <figure class="table-figure" aria-labelledby="tab-3d-class-counts-caption">
          <figcaption id="tab-3d-class-counts-caption"><strong>Table:</strong> Number of objects for 3D object detection</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:3d_class_counts">
              <thead>
                <tr><th></th><th>Ambulance</th><th>Construction</th><th>Crossbike</th><th>Walker</th><th>Car</th><th>Firetruck</th><th>Police</th></tr>
              </thead>
              <tbody>
                <tr><th>train</th><td>2,306</td><td>50,486</td><td>28,980</td><td>17,247</td><td>683,332</td><td>1,070</td><td>21,181</td></tr>
                <tr><th>val</th><td>2,060</td><td>2,248</td><td>32,177</td><td>9,703</td><td>419,697</td><td>1,495</td><td>9,803</td></tr>
              </tbody>
            </table>
          </div>
        </figure>

        <p>
          We use camera-only and camera‚ÄìLiDAR configurations and evaluation metrics from nuScenes (mAP, ATE, ASE, AOE, AVE). LiDAR improves localization (lower ATE/AOE).
        </p>
      </article>

      <!-- Lane Detection -->
      <article class="experiment-block" id="lane-detection">
        <h3>Lane Detection</h3>

        <figure class="table-figure" aria-labelledby="tab-topobda-caption">
          <figcaption id="tab-topobda-caption"><strong>Table:</strong> Centerline and Lane Divider Detection Results (TopoBDA)</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:topobda_tacarla">
              <thead><tr><th>Detection Task</th><th>AP<sub>f</sub></th><th>AP<sub>c</sub></th><th>F1<sub>1.5</sub></th></tr></thead>
              <tbody>
                <tr><td>Centerline Detection</td><td>58.2</td><td>60.9</td><td>73.8</td></tr>
                <tr><td>Lane Divider Detection</td><td>N/A</td><td>60.2</td><td>75.6</td></tr>
              </tbody>
            </table>
          </div>
        </figure>

        <!-- BEV figure from figures/ (user requested figures folder included) -->
        <figure class="figure" id="fig:bev_samples">
          <img src="figures/bev_tacarla.png" alt="BEV results (figures/bev_tacarla.png)" data-full="figures/bev_tacarla.png" style="width:100%;max-width:900px;cursor:zoom-in;border-radius:6px;">
          <figcaption>Bird's Eye View (BEV) results ‚Äî GT, Pred and overlay (image loaded from <code>figures/bev_tacarla.png</code>).</figcaption>
        </figure>

        <!-- traffic light samples (images/) and larger display -->
        <figure class="figure tl-samples" role="group" aria-label="Traffic light model outputs">
          <img src="images/tl_red.jpg" alt="Traffic light red example (images/tl_red.jpg)" data-full="images/tl_red.jpg">
          <img src="images/tl_green.jpg" alt="Traffic light green example (images/tl_green.jpg)" data-full="images/tl_green.jpg">
          <figcaption style="width:100%;margin-top:10px;text-align:center;">Outputs of the FCOS traffic light model (red and green examples).</figcaption>
        </figure>
      </article>

      <!-- Traffic Light Detection -->
      <article class="experiment-block" id="traffic-light-detection">
        <h3>Traffic Light Detection</h3>
        <p>The dataset contains <strong>238,780</strong> (train) and <strong>187,987</strong> (val) images with traffic light instances (red/yellow/green). Baseline: FCOS + ResNet-50.</p>

        <figure class="table-figure" aria-labelledby="tab-tlr-caption">
          <figcaption id="tab-tlr-caption"><strong>Table:</strong> Traffic Light Detection results</figcaption>
          <div class="table-responsive">
            <table class="results-table" id="tab:TLR">
              <thead><tr><th>Model</th><th>AP</th><th>AP<sub>50</sub></th></tr></thead>
              <tbody><tr><td>FCOS</td><td>59.5</td><td>88.2</td></tr></tbody>
            </table>
          </div>
        </figure>
      </article>

      <!-- Planning -->
      <article class="experiment-block" id="planning">
        <h3>Planning</h3>

        <!-- planning figure from figures/ (user requested figures folder included) -->
        <figure class="figure">
          <img src="figures/planning.png" alt="Planning comparison (figures/planning.png)" data-full="figures/planning.png" style="width:100%;max-width:900px;border-radius:6px;">
          <figcaption>Waypoints from ground truth, PlanT, Transfuser, and DiffusionDrive (image loaded from <code>figures/planning.png</code>).</figcaption>
        </figure>

        <p>We trained Transfuser, DiffusionDrive and PlanT with standard settings (ResNet-34 backbone, camera + LiDAR). Open-loop (ADE/FDE/AHE/FHE) and closed-loop evaluations provided.</p>

      </article>

    </div>
  </section>

  <!-- Visualization (images/ folder) -->
  <section class="section" id="visualization">
    <div class="container">
      <h2>Visualization</h2>
      <p class="small-muted">Three data visualization images (in this order) from the <code>images/</code> folder.</p>

      <div class="viz-gallery" role="list">
        <div class="viz-item" role="listitem">
          <img src="images/select_episode.png" alt="Select Episode (images/select_episode.png)" data-full="images/select_episode.png" loading="lazy">
          <div class="viz-caption">Select Episode (interface screenshot)</div>
        </div>

        <div class="viz-item" role="listitem">
          <img src="images/bev.png" alt="BEV Visualization (images/bev.png)" data-full="images/bev.png" loading="lazy">
          <div class="viz-caption">BEV Visualization</div>
        </div>

        <div class="viz-item" role="listitem">
          <img src="images/front.png" alt="Front Camera View (images/front.png)" data-full="images/front.png" loading="lazy">
          <div class="viz-caption">Front Camera View</div>
        </div>
      </div>
    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="citation">
    <div class="container">
      <h2>Citation</h2>
      <div class="citation-box">
        <pre id="citation-text">@inproceedings{2025tacarla,
  title   = {TaCarla: A comprehensive benchmarking dataset for end-to-end autonomous driving},
  author  = {TaCarla Research Team},
  year    = {2025}
}</pre>
        <div style="display:flex;flex-direction:column;gap:8px; margin-top:8px;">
          <button class="btn btn-primary" id="copyCitation">üìã Copy Citation</button>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer" style="padding:18px 0;background:#fff;border-top:1px solid #eee;">
    <div class="container" style="display:flex;gap:12px;justify-content:space-between;align-items:center;">
      <div>¬© 2025 TaCarla Team</div>
      <div>Contact: <a href="mailto:team@tacarla.example">team@tacarla.example</a></div>
    </div>
  </footer>

  <!-- Image modal -->
  <div id="imageModal" class="image-modal" role="dialog" aria-hidden="true">
    <div class="close-modal" id="closeModal" title="Close">&times;</div>
    <img id="modalImage" src="" alt="Full size" />
  </div>

  <!-- Inline script: image modal + copy citation -->
  <script>
    // image modal
    (function(){
      document.querySelectorAll('img[data-full]').forEach(img=>{
        img.style.cursor = 'zoom-in';
        img.addEventListener('click', ()=>{
          const modal = document.getElementById('imageModal');
          const modalImg = document.getElementById('modalImage');
          modalImg.src = img.dataset.full || img.src;
          modal.classList.add('active');
          modal.setAttribute('aria-hidden','false');
          document.body.style.overflow = 'hidden';
        });
      });
      const closeBtn = document.getElementById('closeModal');
      if (closeBtn) closeBtn.addEventListener('click', ()=>{
        const modal = document.getElementById('imageModal');
        modal.classList.remove('active');
        modal.setAttribute('aria-hidden','true');
        document.getElementById('modalImage').src = '';
        document.body.style.overflow = '';
      });
      document.addEventListener('keydown', (e)=>{ if (e.key==='Escape'){
        const modal=document.getElementById('imageModal'); if(modal && modal.classList.contains('active')){
          modal.classList.remove('active'); modal.setAttribute('aria-hidden','true'); document.getElementById('modalImage').src=''; document.body.style.overflow='';
        }
      }});
      // clicking outside image closes modal
      const imgModal = document.getElementById('imageModal');
      imgModal && imgModal.addEventListener('click', (e)=>{ if (e.target === imgModal){ imgModal.classList.remove('active'); imgModal.setAttribute('aria-hidden','true'); document.getElementById('modalImage').src=''; document.body.style.overflow=''; } });
    })();

    // copy citation
    (function(){
      const btn = document.getElementById('copyCitation');
      if (!btn) return;
      btn.addEventListener('click', async ()=>{
        const text = document.getElementById('citation-text').innerText;
        try {
          await navigator.clipboard.writeText(text);
          btn.textContent = '‚úÖ Copied!';
          setTimeout(()=> btn.textContent = 'üìã Copy Citation', 1500);
        } catch (err){
          alert('Could not copy automatically. Here is the citation:\\n\\n' + text);
        }
      });
    })();
  </script>
</body>
</html>
